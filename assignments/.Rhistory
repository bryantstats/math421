install.packages('rmarkdown')
Is this as good as any the other one is better a little butthis is definiely better to look at but not the best to type on.  The suface book is a little bit better!
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(rtweet)
library(tidytext)
library(ggpubr)
library(tidyverse)
library(knitr)
library(lubridate)
auth_setup_default()
auth_setup_default()
rtweet_user()
vignette('auth', 'rtweet')
library("rtweet")
auth_as(token, "academic_token")
client <- rtweet_client(app = "my app")
install.packages("RedditExtractoR")
# install.packages("RedditExtractoR")
library(RedditExtractoR)
top_cats_urls <- find_thread_urls(subreddit="cats", sort_by="top")
str(top_cats_urls)
top_cats_urls
class(top_cats_urls)
View(top_cats_urls)
df = as_tibble(top_cats_urls)
df
df = as_tibble(top_cats_urls)
summary(df$date_utc)
df$date_utc = as.Date(df$date_utc)
summary(df$date_utc)
df %>% filter(date_utc=='2023-10-11')
df %>% filter(date_utc=='2023-10-10')
df %>% filter(date_utc=='2023-10-9')
df %>% filter(date_utc=='2023-10-09')
table(df$date_utc)
as.POSIXct(df$timestamp)
library(lubridate)
hms(df$timestamp)
hms(as.POSIXct(df$timestamp))
dmy_hms(as.POSIXct(df$timestamp))
as.POSIXct(df$timestamp)
format(date, format = "%H:%M:%S")
x = as.POSIXct(df$timestamp)
format(x, format = "%H:%M:%S")
x
# install.packages("RedditExtractoR")
library(RedditExtractoR)
top_cats_urls <- find_thread_urls(subreddit="cats", sort_by="top")
df = as_tibble(top_cats_urls)
# craete a date_time variable
df$date_time = as.POSIXct(df$timestamp)
df$time = format(df$date_time, format = "%H:%M:%S")
# create a date_time variable
df$date_time = as.POSIXct(df$timestamp)
df$time = format(df$date_time, format = "%H:%M:%S")
df
install.packages("hms")
install.packages("hms")
ts_plot(df, "hours") +
labs(x = NULL, y = NULL,
title = "Frequency of tweets by time",
subtitle = paste0(format(min(df$created_at), "%d %B %Y"), " to ", format(max(df$created_at),"%d %B %Y")),
caption = "Data collected from Twitter's REST API via rtweet") +
theme_minimal()
ts_plot(df, "hours")
ts_plot(df)
?ts_plot
plot(df$time)
df
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(type, word, sort = TRUE) %>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(type, word, sort = TRUE) %>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(type, word, sort = TRUE)
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords())
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
head(20) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
?count
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE)
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
mutate(n/sum(n))
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
mutate(relative_freq = n/sum(n))
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
mutate(relative_freq = 100*n/sum(n))
library(RColorBrewer)
pal <- brewer.pal(8,"Dark2")
library(wordcloud)
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
df
library(RColorBrewer)
pal <- brewer.pal(8,"Dark2")
library(wordcloud)
df %>%
unnest_tokens(input = title, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
filter(!word %in% c('cat','cats','s')) %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
knitr::opts_chunk$set(message = FALSE)
df2 = get_thread_content('https://www.reddit.com/r/programming/')
df2 = get_thread_content('https://www.reddit.com/r/financialindependence/comments/17pzwmg/health_insurance_from_55_to_65_question/')
df2
View(df2)
df2$comments
df2$comments$comment
View(df2$comments$comment)
df2$comments$comment
str(df2$comments$comment)
df2$comments$comment[1]
df2$comments$comment[2]
df$time
hms(df$time)
df$time
mean(df$time)
class(df$time)
format(as.POSIXct(x), format = "%H")
format(as.POSIXct(df$time), format = "%H")
format(as.POSIXct(df$timestamp), format = "%H")
as.numeric(format(as.POSIXct(df$timestamp), format = "%H"))
y  = as.numeric(format(as.POSIXct(df$timestamp), format = "%H"))
mean(y)
summary(y)
hist(y)
df3 <- find_thread_urls(keywords = 'Bryant University', sort_by="top")
df3
View(df3)
df3 <- find_thread_urls(keywords = 'Bryant University', sort_by="top", subreddit = 'college')
View(f3)
View(df3)
df3 <- find_thread_urls(sort_by="top", subreddit = 'college')
View(df3)
df3$title[1]
df3$text[1]
library(RedditExtractoR)
library(tidytext)
library(ggpubr)
library(tidyverse)
library(knitr)
library(lubridate)
write_csv(df3, reddit_college.csv)
write_csv(reddit_college.csv, df3)
?write_csv
write_csv(df3, "reddit_college.csv")
df = read_csv("reddit_college.csv")
df
df$timestamp
decimate_time()
library(lubridate)
decimate_time()
decimate_time
df
df$date_time = as.POSIXct.numeric(df$timestamp)
df$date_time
df$date_time[1]
class(df$date_time[1])
as.numeric(df$date_time[1])
df = read_csv("reddit_college.csv")
df$date_time = as.POSIXct(df$timestamp)
df$date_time = as.POSIXct(df$timestamp)
df$date_time[1]
class(df$date_time[1])
df$date_time = as.POSIXct(df$timestamp)
df$time = format(df$date_time, format = "%H:%M:%S")
df$time
as.numeric(df$time)
df$time = format(df$date_time, format = "hours")
df$time
df$date_time = as.POSIXct(df$timestamp)
df$time = format(df$date_time, format = "Hours")
df$time
as.numeric(hms(c("00:01:38", "01:18:30")), "minutes")
as.numeric(hms(c("00:01:38"))
as.numeric(hms(c("00:01:38")
as.numeric(hms(c("00:01:38")))
?hms
as.numeric(hms(c("00:01:38", "01:18:30")), "hours")
as.numeric(hms(c("00:01:38")), "hours")
as.numeric("00:01:38", "hours")
as.numeric(hms(c("00:01:38")), "hours"))
df$date_time = as.POSIXct(df$timestamp)
df$hours = as.numeric(hms(df$date_time, "hours"))
df$date_time
hour(df$date_time)
hms(df$date_time)
format(as.POSIXct(df$date_time), format = "%H:%M:%S")
hms(df$time)
hms(df$time, "hours")
as.numeric(hms(c("00:01:38", "01:18:30")), "minutes")
as.numeric(hms(c(df$time), "minutes")
as.numeric(hms(df$time, "minutes"))
(hms(df$time, "minutes")
df$text
as.numeric(hms(c("00:01:38", "01:18:30")), "minutes")
as.numeric(hms(c("00:01:38", "01:18:30")), "minutes")
knitr::opts_chunk$set(message = FALSE)
library(caret)
library(themis)
library(textrecipes)
df = read_csv('reddit_college.csv')
setwd("C:/Users/sonou/Dropbox/git/math421/assignments")
library(caret)
library(themis)
library(textrecipes)
df = read_csv('reddit_college.csv')
library(caret)
library(themis)
library(textrecipes)
library(tidyverse)
df = read_csv('reddit_college.csv')
df$threads = case_when(df$comments > 2 ~ "Long",
TRUE ~ "Short")
# Select data and set target
df <- df %>%
mutate(target = threads) %>%
select(target, title)
# Convert text data to numeric variables
a <- recipe(target~title,
data = df) %>%
step_tokenize(title) %>%
step_tokenfilter(title, max_tokens = 10) %>%
step_tfidf(title) %>%
step_normalize(all_numeric_predictors()) %>%
step_smote(target) %>%
prep()
df <- juice(a)
# Using Caret for modeling
set.seed(2021)
splitIndex <- createDataPartition(df$target, p = .7,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]
forest_ranger <- train(target~., data=df_train,
method = "ranger")
pred <- predict(forest_ranger, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]
d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
library(caret)
library(themis)
library(textrecipes)
library(tidyverse)
df = read_csv('reddit_college.csv')
df$threads = case_when(df$comments > 2 ~ "Long",
TRUE ~ "Short")
# Select data and set target
df <- df %>%
mutate(target = threads) %>%
select(target, title)
# Convert text data to numeric variables
a <- recipe(target~title,
data = df) %>%
step_tokenize(title) %>%
step_tokenfilter(title, max_tokens = 20) %>%
step_tfidf(title) %>%
step_normalize(all_numeric_predictors()) %>%
step_smote(target) %>%
prep()
df <- juice(a)
# Using Caret for modeling
set.seed(2021)
splitIndex <- createDataPartition(df$target, p = .7,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]
forest_ranger <- train(target~., data=df_train,
method = "ranger")
pred <- predict(forest_ranger, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]
d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
library(caret)
library(themis)
library(textrecipes)
library(tidyverse)
df = read_csv('reddit_college.csv')
df$threads = case_when(df$comments > 2 ~ "Long",
TRUE ~ "Short")
# Select data and set target
df <- df %>%
mutate(target = threads) %>%
select(target, title)
# Convert text data to numeric variables
a <- recipe(target~title,
data = df) %>%
step_tokenize(title) %>%
step_tokenfilter(title, max_tokens = 30) %>%
step_tfidf(title) %>%
step_normalize(all_numeric_predictors()) %>%
step_smote(target) %>%
prep()
df <- juice(a)
# Using Caret for modeling
set.seed(2021)
splitIndex <- createDataPartition(df$target, p = .7,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]
forest_ranger <- train(target~., data=df_train,
method = "ranger")
pred <- predict(forest_ranger, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]
d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
library(caret)
library(themis)
library(textrecipes)
library(tidyverse)
df = read_csv('reddit_college.csv')
df$threads = case_when(df$comments > 2 ~ "Long",
TRUE ~ "Short")
# Select data and set target
df <- df %>%
mutate(target = threads) %>%
select(target, title)
# Convert text data to numeric variables
a <- recipe(target~title,
data = df) %>%
step_tokenize(title) %>%
step_tokenfilter(title, max_tokens = 50) %>%
step_tfidf(title) %>%
step_normalize(all_numeric_predictors()) %>%
step_smote(target) %>%
prep()
df <- juice(a)
# Using Caret for modeling
set.seed(2021)
splitIndex <- createDataPartition(df$target, p = .7,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]
forest_ranger <- train(target~., data=df_train,
method = "ranger")
pred <- predict(forest_ranger, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]
d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
nrow(df)
library(caret)
library(themis)
library(textrecipes)
library(tidyverse)
df = read_csv('reddit_college.csv')
df$threads = case_when(df$comments > 2 ~ "Long",
TRUE ~ "Short")
# Select data and set target
df <- df %>%
mutate(target = threads) %>%
select(target, title)
# Convert text data to numeric variables
a <- recipe(target~title,
data = df) %>%
step_tokenize(title) %>%
step_tokenfilter(title, max_tokens = 100) %>%
step_tfidf(title) %>%
step_normalize(all_numeric_predictors()) %>%
step_smote(target) %>%
prep()
df <- juice(a)
# Using Caret for modeling
set.seed(2023)
splitIndex <- createDataPartition(df$target, p = .7,
list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]
forest_ranger <- train(target~., data=df_train,
method = "ranger")
pred <- predict(forest_ranger, df_test)
cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]
d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
