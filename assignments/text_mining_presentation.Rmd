---
title: "Text Mining"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  word_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


```{r}
library(tidyverse)
library(tidytext)
library(knitr)
df <- readr::read_csv('https://bryantstats.github.io/math421/data/netflix_titles.csv')
df %>% 
  head(2) %>% 
  kable()
```

## Convert sentences to words

- One column of sentence is converted to multiple columns of words

```{r}
df %>% 
  filter(title=='Avengers: Infinity War') %>% 
  select(title, description) %>% 
  kable()
```

```{r}
df %>% 
  filter(title=='Avengers: Infinity War') %>% 
  select(title, description) %>% 
  unnest_tokens(input = description, output = word) %>% 
  kable()
```

## Remove Stop Words

```{r}
# Remove rows with stop words
df %>% 
  filter(title=='Avengers: Infinity War') %>% 
  select(title, description) %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  kable()
```

## Then plot it

### Word Frequency


```{r}
df %>%
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(type, word, sort = TRUE) %>% 
  group_by(type) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = type)) %>%
  ggplot(aes(n, word, fill = type)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~type, scales = "free") +
  labs(x = "Contribution to sentiment",
       y = NULL)+
  scale_y_reordered() 

```

### Word Cloud

```{r}
library(wordcloud) 
pal <- brewer.pal(8,"Dark2")

df %>%
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(type, word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
```


## Sentiment Analysis


```{r}
# Remove rows with stop words
df %>% 
  filter(title=='Avengers: Infinity War') %>% 
  head(2) %>% 
  select(title, description) %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  kable()
```


### Convert Words to Sentiment

- The sentiment may not exist for some words. 

```{r}
# Remove rows with stop words
df %>% 
  filter(title=='Avengers: Infinity War') %>% 
  select(title, description) %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  inner_join(get_sentiments('nrc')) %>% 
  kable()
```

### Then plot it

```{r}

df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(type, word, sort = TRUE) %>%
    group_by(type) %>% 
    inner_join(get_sentiments("nrc")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(type) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill=type))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')

```


## Modeling


```{r}
library(caret)
library(themis)
library(textrecipes)

df <- read_csv('https://bryantstats.github.io/math421/data/netflix_titles.csv')


# Select data and set target 
df %>% 
  head(2) %>% 
  rename(target = type) %>% 
  select(target, description) %>% 
  kable()
```

## Convert Text to numbers

-  Words are converted to numbers using `tdidf` formula

-  Change `max_tokens` to decide how many words are used

```{r}

df <- read_csv('https://bryantstats.github.io/math421/data/netflix_titles.csv')

df <- df %>% 
  head(2) %>% 
  rename(target = type) %>% 
  select(target, description)
  
# Convert text data to numeric variables
a <- recipe(target~description,
       data = df) %>% 
  step_tokenize(description) %>% 
  step_tokenfilter(description, max_tokens = 10) %>% 
  step_stopwords(description) %>% 
  step_tfidf(description) %>% 
  prep()

d1 <- juice(a)
kable(d1)
```

- Then normalize (step_normalize) the data:

### Normalizing


```{r}
df <- read_csv('https://bryantstats.github.io/math421/data/netflix_titles.csv')

df <- df %>% 
  head(2) %>% 
  rename(target = type) %>% 
  select(target, description)
  
# Convert text data to numeric variables
a <- recipe(target~description,
       data = df) %>% 
  step_tokenize(description) %>% 
  step_tokenfilter(description, max_tokens = 10) %>% 
  step_stopwords(description) %>% 
  step_tfidf(description) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  prep()

d1 <- juice(a)
kable(d1)
```


- Then balance (step_smote) the data for modeling


```{r, eval=FALSE}
df <- read_csv('https://bryantstats.github.io/math421/data/netflix_titles.csv')



# Select data and set target 
df <- df %>% 
  rename(target = type) %>% 
  select(target, description) 

# Convert text data to numeric variables
a <- recipe(target~description,
       data = df) %>% 
  step_tokenize(description) %>% 
  step_tokenfilter(description, max_tokens = 10) %>% 
  step_tfidf(description) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(target) %>% 
  prep()
df <- juice(a)

# Using Caret for modeling
set.seed(2021)
splitIndex <- createDataPartition(df$target, p = .2, 
                                  list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]

forest_ranger <- train(target~., data=df_train, 
                        method = "ranger")

pred <- predict(forest_ranger, df_test)

cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]


```





