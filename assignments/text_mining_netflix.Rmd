---
title: "Text Mining with Netflix"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: false
    
    theme: united
  word_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
df <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')
```


## 1. Word Frequency

- A token is a meaningful unit of text. 

- One row of text will be converted to multiple rows of tokens. 

### By TV Show

```{r}
df %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(type, word, sort = TRUE) %>% 
  filter(type=='TV Show') %>% 
  head(10) %>% 
  ggplot(aes(x = n, y = reorder(word, n))) +
  geom_col() +
  labs(y = '', x = 'Frequency')

```

### TV Show vs. Movies

```{r}
df %>%
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(type, word, sort = TRUE) %>% 
  group_by(type) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = type)) %>%
  ggplot(aes(n, word, fill = type)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~type, scales = "free") +
  labs(x = "Frequency",
       y = NULL)+
  scale_y_reordered() 

```

### By Century

```{r}
df %>%
  mutate(century = if_else(release_year>=2000, '21','20')) %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(century, word, sort = TRUE) %>% 
  group_by(century) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = century)) %>%
  ggplot(aes(n, word, fill = century)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~century, scales = "free") +
  labs(x = "Frequency",
       y = NULL)+
  scale_y_reordered() 

```


## 2. Word Cloud

### Total Word Cloud
```{r}
library(wordcloud) 
pal <- brewer.pal(8,"Dark2")

df %>%
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
```

### By Movie

```{r}
library(wordcloud) 
pal <- brewer.pal(8,"Dark2")

df %>%
  filter(type =='Movie') %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(type, word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))

```

## 3. Sentiment Analysis

### By `bing`


```{r}
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(type, word, sort = TRUE) %>%
    group_by(type) %>% 
    inner_join(get_sentiments("bing")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(type) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(type, n, fill=sentiment))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')
```

```{r}
df %>%
    mutate(century = if_else(release_year>=2000, '21','20')) %>% 
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(century, word, sort = TRUE) %>%
    group_by(century) %>% 
    inner_join(get_sentiments("bing")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(century) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(century, n, fill=sentiment))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')
```


### By `nrc`

```{r}
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(type, word, sort = TRUE) %>%
    group_by(type) %>% 
    inner_join(get_sentiments("nrc")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(type) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill=type))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')

```

### By `afinn`

```{r}
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(type, word, sort = TRUE) %>%
    group_by(type) %>% 
    inner_join(get_sentiments("afinn")) %>%
    mutate(sentiment = value) %>% 
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(type) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(type, n, fill=factor(sentiment)))+geom_col(position = 'dodge')+
    labs(y='Relative Frequency', fill = 'Sentiment', x = '')
```


```{r}
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(type, word, sort = TRUE) %>%
    group_by(type) %>% 
    inner_join(get_sentiments("afinn")) %>%
    mutate(sentiment = value) %>% 
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(type) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill= type))+geom_col(position = 'dodge')+
    labs(y='Relative Frequency', x ='')
```


```{r}
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(rating, word, sort = TRUE) %>%
    group_by(rating) %>% 
    inner_join(get_sentiments("bing")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(rating) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(rating, n, fill=sentiment))+geom_col(position = 'dodge')+
    labs(y='Relative Frequency', x ='')
```


## 4. Modeling

We build a model to predict the type of a video (movies vs. tv show) from its description.  First we need to convert the description (text) to numeric values. 

```{r}
library(caret)
set.seed(2020)
library(themis)

library(textrecipes)

names(df)[names(df)=='type'] <- 'target'

df <- df %>% select(target, description)

a <- recipe(target~description,
       data = df) %>% 
  step_tokenize(description) %>% 
  step_tokenfilter(description, max_tokens = 5) %>% 
  step_tfidf(description) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(target) %>% 
  prep()

df <- juice(a)

splitIndex <- createDataPartition(df$target, p = .05, 
                                  list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]

forest_ranger <- train(target~., data=df_train, 
                        method = "ranger")

pred <- predict(forest_ranger, df_test)

cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]

d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot

```

```{r}
library(caret)
set.seed(2020)
library(themis)

df <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')

df <- df %>% 
  mutate(century = if_else(release_year>=2020, '21','20')) %>% 
  rename(target = type)
  


df <- df %>% select(target, description)
library(textrecipes)

a <- recipe(target~description,
       data = df) %>% 
  step_tokenize(description) %>% 
  step_tokenfilter(description, max_tokens = 15) %>% 
  step_tfidf(description) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(target) %>% 
  prep()

df <- juice(a)


splitIndex <- createDataPartition(df$target, p = .5, 
                                  list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]

forest_ranger <- train(target~., data=df_train, 
                        method = "ranger")

pred <- predict(forest_ranger, df_test)

cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]

d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot

```



```{r}
library(caret)
set.seed(2020)
library(themis)

df <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')

df <- df %>% mutate(century = case_when(release_year>=2020 ~ '21',
                                        release_year>=1950 ~ 'second_half_20',
                                        TRUE ~ 'first_half_20'))
names(df)[names(df)=='century'] <- 'target'

df <- df %>% select(target, description)
library(textrecipes)

a <- recipe(target~description,
       data = df) %>% 
  step_tokenize(description) %>% 
  step_tokenfilter(description, max_tokens = 15) %>% 
  step_tfidf(description) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(target) %>% 
  prep()

df <- juice(a)


splitIndex <- createDataPartition(df$target, p = .5, 
                                  list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]

forest_ranger <- train(target~., data=df_train, 
                        method = "ranger")

pred <- predict(forest_ranger, df_test)

cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]

d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot

```