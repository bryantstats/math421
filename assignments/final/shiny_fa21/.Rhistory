anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
d1 %>% head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
library(wordcloud)
pal <- brewer.pal(8,"Dark2")
d1 %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(tidyverse)
user_reviews <- readr::read_tsv("data/user_reviews.tsv")
user_reviews %>%
count(grade) %>%
ggplot(aes(grade, n)) +
geom_col(fill = "midnightblue", alpha = 0.7)
d1
d1 <- df %>%
unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords())
d1
df %>%
unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
count(type, word, sort = TRUE) %>%
group_by(type) %>%
inner_join(get_sentiments("bing"))
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
x <- def.chunk.hook(x, options)
ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
df %>%
unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
inner_join(get_sentiments("nrc"))
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(knitr)
library(kableExtra)
df <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')
df %>% head(5) %>% select(title, release_year, description) %>% kable()
df %>%
unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
inner_join(get_sentiments("nrc"))
df %>%
unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
inner_join(get_sentiments("bing"))
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
kable()%>% kable_styling(font_size = 12) %>%
inner_join(get_sentiments("bing"))
d1 <- df %>%
filter(title=='30 Days of Luxury') %>%
select(title, description)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
kable()%>% kable_styling(font_size = 12) %>%
inner_join(get_sentiments("bing"))
d1
d1 <- df %>%
filter(title=='30 Days of Luxury') %>%
select(title, description)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("bing"))
d1 <- df %>%
filter(title=='30 Days of Luxury') %>%
select(title, description)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("bing"))
d1
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords())
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("bing"))
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("nrc"))
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("afinn"))
d1 <- df %>%
filter(title=='21') %>%
select(title, description)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("nrc"))
d1 <- df %>%
filter(title=='21') %>%
select(title, description)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("bing"))
get_sentiments("bing")
get_sentiments("bing") %>% filter(word='help')
get_sentiments("bing") %>% filter(word=='help')
get_sentiments("bing") %>% filter(word=='son')
get_sentiments("bing") %>% filter(word=='abort')
get_sentiments("nrc") %>% filter(word=='abort')
get_sentiments("bing") %>% filter(word=='abort')
get_sentiments("afinn") %>% filter(word=='abort')
d1 <- df %>%
filter(title=='21') %>%
select(title, description)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("nrc"))
get_sentiments("afinn") %>% filter(word=='brilliant')
get_sentiments("nrc") %>% filter(word=='brilliant')
d1 <- df %>%
filter(title=='300') %>%
select(title, description)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("nrc"))
View(df)
d1 <- df %>%
filter(title=='Avengers: Infinity War') %>%
select(title, description)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("nrc"))
d1 <- df %>%
filter(title=='Avengers: Infinity War') %>%
select(title, description)
d1
d1 %>% unnest_tokens(input = description, output = word) %>%
kable() %>% kable_styling(font_size = 12)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
kable() %>% kable_styling(font_size = 12)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("bing")) %>%
kable()
d1 %>% unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
inner_join(get_sentiments("afinn")) %>%
kable()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(tidytext)
library(knitr)
df <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')
df %>%
head(5) %>%
kable(df)
library(tidyverse)
library(tidytext)
library(knitr)
df <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')
df %>%
head(5) %>%
kable()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(tidytext)
df <- readr::read_csv('data/netflix_titles.csv')
df %>%
head(1) %>%
select(title, description) %>%
unnest_tokens(input = description, output = word) %>%
kable()
d1 %>%
count(type, word, sort = TRUE) %>%
head(10) %>%
kable()
df
df
df %>% rename(type=target)
df %>% rename(target=type)
library(caret)
library(themis)
library(textrecipes)
df <- read_csv('data/netflix_titles.csv')
# Select data and set target
df %>%
head(2) %>%
rename(target = type) %>%
select(target, description) %>%
kable()
library(caret)
library(themis)
library(textrecipes)
df <- read_csv('data/netflix_titles.csv')
# Select data and set target
df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
df
df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
df
df %>%
head(2) %>%
mutate(target = type) %>%
select(target, description)
df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
df %>%
head(2) %>%
rename(target = type) %>%
select(target, description) %>%
recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 50) %>%
step_tfidf(description) %>%
prep()
df %>%
head(2) %>%
rename(target = type) %>%
select(target, description) %>%
recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 50) %>%
step_tfidf(description) %>%
prep()
df %>%
head(2) %>%
rename(target = type) %>%
select(target, description) %>%
recipe(target~descriptionf) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 50) %>%
step_tfidf(description) %>%
prep()
df
View(df)
df %>%
filter(title=='Avengers: Infinity War') %>%
select(title, description) %>%
unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
inner_join(get_sentiments('bing'))
df %>%
filter(title=='Avengers: Infinity War') %>%
select(title, description) %>%
unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE) %>%
inner_join(get_sentiments('nrc'))
?get_stopwords
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 50) %>%
get_stopwords() %>%
step_tfidf(description) %>%
prep()
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 50) %>%
get_stopwords(description) %>%
step_tfidf(description) %>%
prep()
?step_stopwords
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 5) %>%
step_tfidf(description) %>%
step_stopwords(description) %>%
step_normalize(all_numeric_predictors()) %>%
prep()
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 50) %>%
step_tfidf(description) %>%
step_stopwords(description) %>%
step_normalize(all_numeric_predictors()) %>%
prep()
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 10) %>%
step_stopwords(description) %>%
step_tfidf(description) %>%
step_normalize(all_numeric_predictors()) %>%
prep()
df %>%
unnest_tokens(input = description, output = word) %>%
anti_join(get_stopwords()) %>%
count(type, word, sort = TRUE) %>%
group_by(type) %>%
inner_join(get_sentiments("nrc")) %>%
filter(!is.na(sentiment)) %>%
count(sentiment, sort = TRUE) %>%
group_by(type) %>%
mutate(n = n/sum(n)) %>%
ggplot(aes(sentiment, n, fill=type))+geom_col(position = 'fill')+
labs(y='Relative Frequency', x ='')
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 10) %>%
step_stopwords(description) %>%
step_tfidf(description) %>%
step_normalize(all_numeric_predictors()) %>%
step_smote(target) %>%
prep()
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 10) %>%
step_stopwords(description) %>%
step_tfidf(description) %>%
prep()
d1 <- juice(a)
kable(d1)
library(tidyverse)
library(tidytext)
library(knitr)
df <- readr::read_csv('data/netflix_titles.csv')
df %>%
head(2) %>%
kable()
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
head(2) %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 10) %>%
step_stopwords(description) %>%
step_tfidf(description) %>%
prep()
d1 <- juice(a)
kable(d1)
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 10) %>%
step_stopwords(description) %>%
step_tfidf(description) %>%
step_normalize(all_numeric_predictors()) %>%
step_smote(target) %>%
prep()
df
df <- read_csv('data/netflix_titles.csv')
df <- df %>%
rename(target = type) %>%
select(target, description)
# Convert text data to numeric variables
a <- recipe(target~description,
data = df) %>%
step_tokenize(description) %>%
step_tokenfilter(description, max_tokens = 50) %>%
step_tfidf(description) %>%
step_normalize(all_numeric_predictors()) %>%
step_smote(target) %>%
prep()
d1 <- juice(a)
kable(d1)
1,990 *.12(82000-1990)
1990 + *.12(82000-1990)
1990 + .12*(82000-1990)
1990 + .12*(81000-1990)
9328+.22*(82000-81050)
1990 + .12*(81000-19900)
1990 + .12*(81000-19900)
9328+.22*(82000-81050)
1990 + .12*(70000-19900)
library(shiny); runApp('C:/Users/sonou/Dropbox/git/math421/shiny_fa21/2.R')
?validate
runApp('C:/Users/sonou/Dropbox/git/math421/shiny_fa21/1.R')
runApp('C:/Users/sonou/Dropbox/git/math421/shiny_fa21/2.R')
runApp('C:/Users/sonou/Dropbox/git/math421/shiny_fa21/1.R')
runApp('C:/Users/sonou/Dropbox/git/math421/shiny_fa21/3.R')
runApp('C:/Users/sonou/Dropbox/git/math421/shiny_fa21/3.R')
runApp('C:/Users/sonou/Dropbox/git/math421/shiny_fa21/3.R')
d = read_csv('who_covid.csv')
setwd("C:/Users/sonou/Dropbox/git/math421/shiny_fa21")
d = read_csv('who_covid.csv')
d
runApp('3.R')
runApp('3.R')
runApp('3.R')
d
runApp('3.R')
runApp('3.R')
runApp('3.R')
runApp('3.R')
runApp('3.R')
runApp('3.R')
runApp('3.R')
d
runApp('3.R')
d
table(d$Country)
d$Country_code
table(d$Country_code)
table(d$Country)
runApp('3.R')
runApp('3.R')
runApp('3.R')
runApp('3.R')
runApp('3.R')
d %>% filter(Country %in% c('United State of America', 'Vietnam'))
d$Country
table(d$Country)
d %>% filter(Country %in% c('United States of America', 'Viet Nam'))
d %>% filter(Country %in% c('United States of America', 'Viet Nam')) %>% gplot(aes(x = Date_reported, y = Cumulative_cases)+
geom_point()
d %>% filter(Country %in% c('United States of America', 'Viet Nam')) %>% gplot(aes(x = Date_reported, y = Cumulative_cases))+
geom_point()
d %>% filter(Country %in% c('United States of America', 'Viet Nam')) %>% ggplot(aes(x = Date_reported, y = Cumulative_cases))+
geom_point()
runApp('3.R')
d %>% filter(Country %in% c('United States of America', 'Viet Nam')) %>% ggplot(aes(x = Date_reported, y = Cumulative_cases))+
geom_point()
runApp('3.R')
runApp('3.R')
runApp('3.R')
runApp('3.R')
names(d)
runApp('3.R')
runApp('3.R')
runApp('3.R')
runApp('3.R')
d
d %>% select_if(is.numeric)
d %>% select_if(is.numeric) %>% names()
runApp('3.R')
runApp('4.R')
runApp('4.R')
runApp('4.R')
runApp('4.R')
runApp('dateRangeInput/4.R')
runApp('dateRangeInput/4.R')
